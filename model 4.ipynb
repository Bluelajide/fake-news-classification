{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26d8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import torch\n",
    "from transformers.file_utils import is_tf_available, is_torch_available\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, RobertaTokenizerFast, RobertaForSequenceClassification, BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c58d7d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6335, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6903</td>\n",
       "      <td>Tehran, USA</td>\n",
       "      <td>\\nI’m not an immigrant, but my grandparents ...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7341</td>\n",
       "      <td>Girl Horrified At What She Watches Boyfriend D...</td>\n",
       "      <td>Share This Baylee Luciani (left), Screenshot o...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>95</td>\n",
       "      <td>‘Britain’s Schindler’ Dies at 106</td>\n",
       "      <td>A Czech stockbroker who saved more than 650 Je...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4869</td>\n",
       "      <td>Fact check: Trump and Clinton at the 'commande...</td>\n",
       "      <td>Hillary Clinton and Donald Trump made some ina...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2909</td>\n",
       "      <td>Iran reportedly makes new push for uranium con...</td>\n",
       "      <td>Iranian negotiators reportedly have made a las...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1357</td>\n",
       "      <td>With all three Clintons in Iowa, a glimpse at ...</td>\n",
       "      <td>CEDAR RAPIDS, Iowa — “I had one of the most wo...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>988</td>\n",
       "      <td>Donald Trump’s Shockingly Weak Delegate Game S...</td>\n",
       "      <td>Donald Trump’s organizational problems have go...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7041</td>\n",
       "      <td>Strong Solar Storm, Tech Risks Today | S0 News...</td>\n",
       "      <td>Click Here To Learn More About Alexandra's Per...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7623</td>\n",
       "      <td>10 Ways America Is Preparing for World War 3</td>\n",
       "      <td>October 31, 2016 at 4:52 am \\nPretty factual e...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1571</td>\n",
       "      <td>Trump takes on Cruz, but lightly</td>\n",
       "      <td>Killing Obama administration rules, dismantlin...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                              title  \\\n",
       "0         8476                       You Can Smell Hillary’s Fear   \n",
       "1        10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2         3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3        10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4          875   The Battle of New York: Why This Primary Matters   \n",
       "5         6903                                        Tehran, USA   \n",
       "6         7341  Girl Horrified At What She Watches Boyfriend D...   \n",
       "7           95                  ‘Britain’s Schindler’ Dies at 106   \n",
       "8         4869  Fact check: Trump and Clinton at the 'commande...   \n",
       "9         2909  Iran reportedly makes new push for uranium con...   \n",
       "10        1357  With all three Clintons in Iowa, a glimpse at ...   \n",
       "11         988  Donald Trump’s Shockingly Weak Delegate Game S...   \n",
       "12        7041  Strong Solar Storm, Tech Risks Today | S0 News...   \n",
       "13        7623       10 Ways America Is Preparing for World War 3   \n",
       "14        1571                   Trump takes on Cruz, but lightly   \n",
       "\n",
       "                                                 text label  \n",
       "0   Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1   Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2   U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3   — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4   It's primary day in New York and front-runners...  REAL  \n",
       "5     \\nI’m not an immigrant, but my grandparents ...  FAKE  \n",
       "6   Share This Baylee Luciani (left), Screenshot o...  FAKE  \n",
       "7   A Czech stockbroker who saved more than 650 Je...  REAL  \n",
       "8   Hillary Clinton and Donald Trump made some ina...  REAL  \n",
       "9   Iranian negotiators reportedly have made a las...  REAL  \n",
       "10  CEDAR RAPIDS, Iowa — “I had one of the most wo...  REAL  \n",
       "11  Donald Trump’s organizational problems have go...  REAL  \n",
       "12  Click Here To Learn More About Alexandra's Per...  FAKE  \n",
       "13  October 31, 2016 at 4:52 am \\nPretty factual e...  FAKE  \n",
       "14  Killing Obama administration rules, dismantlin...  REAL  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_news = pd.read_csv('fake_or_real_news.csv')\n",
    "print(data_news.shape)\n",
    "data_news[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4a1685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code segment parses the data_news dataset into a more manageable format\n",
    "\n",
    "titles = []\n",
    "tokenized_titles = []\n",
    "sequence_labels = data_news['label']\n",
    "\n",
    "title, tokenized_title =  [], []\n",
    "for news in data_news['title']:\n",
    "    title.append(news)\n",
    "    tokenized_title.append(news.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf5855b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('You Can Smell Hillary’s Fear',\n",
       " ['You', 'Can', 'Smell', 'Hillary’s', 'Fear'],\n",
       " 'FAKE')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python list for each news\n",
    "title[0], tokenized_title[0], sequence_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b44808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FAKE', 'REAL']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sequence_labels = list(set(sequence_labels))\n",
    "unique_sequence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a93b4893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'titles': 'One chart that shows why the Republican Party was ready for Donald Trump',\n",
       " 'label': 1,\n",
       " 'tokens': ['One',\n",
       "  'chart',\n",
       "  'that',\n",
       "  'shows',\n",
       "  'why',\n",
       "  'the',\n",
       "  'Republican',\n",
       "  'Party',\n",
       "  'was',\n",
       "  'ready',\n",
       "  'for',\n",
       "  'Donald',\n",
       "  'Trump']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert sequence_labels to indices:\n",
    "label_indices = [unique_sequence_labels.index(l) for l in sequence_labels]\n",
    "\n",
    "news_dataset = Dataset.from_dict(\n",
    "    dict(\n",
    "        titles=title, \n",
    "        label=label_indices,\n",
    "        tokens=tokenized_title,\n",
    "    )\n",
    ")\n",
    "news_dataset = news_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "news_dataset['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53039ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0af3ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"titles\"], truncation=True, padding=True)# truncation=True makes sure to exludes instances with more 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be8540e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0c395cd0404a1390406006a74e5672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e3c2f790434f9ebddbb1e7c366b6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# go over all our data set, tokenize them\n",
    "seq_clf_tokenized_news = news_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3f5f051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'titles': 'One chart that shows why the Republican Party was ready for Donald Trump',\n",
       " 'label': 1,\n",
       " 'tokens': ['One',\n",
       "  'chart',\n",
       "  'that',\n",
       "  'shows',\n",
       "  'why',\n",
       "  'the',\n",
       "  'Republican',\n",
       "  'Party',\n",
       "  'was',\n",
       "  'ready',\n",
       "  'for',\n",
       "  'Donald',\n",
       "  'Trump'],\n",
       " 'input_ids': [101,\n",
       "  2028,\n",
       "  3673,\n",
       "  2008,\n",
       "  3065,\n",
       "  2339,\n",
       "  1996,\n",
       "  3951,\n",
       "  2283,\n",
       "  2001,\n",
       "  3201,\n",
       "  2005,\n",
       "  6221,\n",
       "  8398,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_clf_tokenized_news['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c7d1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31e249f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_clf_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', \n",
    "                                                                         num_labels=len(unique_sequence_labels),)\n",
    "\n",
    "# set an index -> label dictionary\n",
    "sequence_clf_model.config.id2label = {i: l for i, l in enumerate(unique_sequence_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c4d73df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"FAKE\",\n",
       "    \"1\": \"REAL\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": null,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"problem_type\": \"single_label_classification\",\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.51.3\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_clf_model.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "899bf63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):  # common method to take in logits and calculate accuracy of the eval set\n",
    "    logits, labels = eval_pred   # logit and label are returning from training loop\n",
    "    predictions = np.argmax(logits, axis=-1) \n",
    "    return metric.compute(predictions=predictions, references=labels) # compute the accuracy\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "def compute_metrics_binary(eval_pred):\n",
    "    \"\"\"metrics for binary classification\"\"\"\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Calculate the AUC score\n",
    "    auc_score = roc_auc_score(labels, preds)\n",
    "\n",
    "    # Calculate the accuracy, true positive, false positive, false negative, and true negative values\n",
    "    acc = metric.compute(predictions=preds, references=labels)\n",
    "    tp = ((preds >= 0.5) & (labels == 1)).sum()\n",
    "    fp = ((preds >= 0.5) & (labels == 0)).sum()\n",
    "    fn = ((preds < 0.5) & (labels == 1)).sum()\n",
    "    tn = ((preds < 0.5) & (labels == 0)).sum()\n",
    "\n",
    "    # Calculate the precision, recall, and F1 score\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return {\n",
    "        'Validation Accuracy': acc['accuracy'],\n",
    "        'Validation Precision': auc_score,\n",
    "        'Validation AUC': precision,\n",
    "        'Validation Recall': recall,\n",
    "        'Validation F1_Score': f1_score,\n",
    "        'Validation TP': tp,\n",
    "        'Validation FP': fp,\n",
    "        'Validation FN': fn,\n",
    "        'Validation TN': tn,\n",
    "    }\n",
    "\n",
    "#####################################################\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_metrics_multiclass(eval_pred):\n",
    "    \"\"\"metrics for multiclass classification\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    report = classification_report(labels, preds, output_dict=True)\n",
    "    acc_score = report['accuracy']\n",
    "    pre_score = report['macro avg']['precision']\n",
    "    rcl_score = report['macro avg']['recall']\n",
    "    f1_score = report['macro avg']['f1-score']\n",
    "\n",
    "    return {\n",
    "        'Validation Accuracy': acc_score,\n",
    "        'Validation Macro Recall': rcl_score,\n",
    "        'Validation Macro Precision': pre_score,        \n",
    "        'Validation Macro F1_Score': f1_score,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71df4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "# Training argument\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert_news_clf/results\", # Local directory to save check point of our model as fitting\n",
    "    num_train_epochs=epochs,         # minimum of two epochs\n",
    "    per_device_train_batch_size=32,  # batch size for training and evaluation, it common to take around 32, \n",
    "    per_device_eval_batch_size=32,   # sometimes less or more, The smaller batch size, the more change model update \n",
    "    load_best_model_at_end=True,     # Even if we overfit the model by accident, load the best model through checkpoint\n",
    "    \n",
    "    # some deep learning parameters that the trainer is able to take in\n",
    "    warmup_steps = len(seq_clf_tokenized_news['train']) // 5,  # learning rate scheduler by number of warmup steps\n",
    "    weight_decay = 0.05,    # weight decay for our learning rate schedule (regularization)\n",
    "    \n",
    "    logging_steps = 1,  # Tell the model minimum number of steps to log between (1 means logging as much as possible)\n",
    "    log_level = 'info',\n",
    "    eval_strategy = 'epoch', # It is \"steps\" or \"epoch\", we choose epoch: how many times to stop training to test\n",
    "    eval_steps = 50,\n",
    "    save_strategy = 'epoch'  # save a check point of our model after each epoch\n",
    ")\n",
    "\n",
    "# Define the trainer:\n",
    "trainer = Trainer(\n",
    "    model=sequence_clf_model,   # take our model (sequence_clf_model)\n",
    "    args=training_args,         # we just set it above\n",
    "    train_dataset=seq_clf_tokenized_news['train'], # training part of dataset\n",
    "    eval_dataset=seq_clf_tokenized_news['test'],   # test (evaluation) part of dataset\n",
    "    compute_metrics=compute_metrics_binary,    # This part is optional but we want to calculate accuracy of our model \n",
    "    data_collator=data_collator         # data colladior with padding. Infact, we may or may not need a data collator\n",
    "                                        # we can check the model to see how it lookes like with or without the collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f6d7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tokens, titles. If tokens, titles are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1267\n",
      "  Batch size = 32\n",
      "c:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.0193748474121094,\n",
       " 'eval_model_preparation_time': 0.001,\n",
       " 'eval_Validation Accuracy': 0.1239147592738753,\n",
       " 'eval_Validation Precision': 0.1243366848205558,\n",
       " 'eval_Validation AUC': 0.12912912912912913,\n",
       " 'eval_Validation Recall': 0.1396103896103896,\n",
       " 'eval_Validation F1_Score': 0.13416536661466458,\n",
       " 'eval_Validation TP': 86,\n",
       " 'eval_Validation FP': 580,\n",
       " 'eval_Validation FN': 530,\n",
       " 'eval_Validation TN': 71,\n",
       " 'eval_runtime': 14.4483,\n",
       " 'eval_samples_per_second': 87.692,\n",
       " 'eval_steps_per_second': 2.768}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get initial metrics: evaluation on test set\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42f93922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tokens, titles. If tokens, titles are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 5,068\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,590\n",
      "  Number of trainable parameters = 66,955,010\n",
      "c:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\quang\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2246\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2247\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2248\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2249\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2250\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\quang\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2558\u001b[0m )\n\u001b[0;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2566\u001b[0m ):\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\quang\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3782\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   3780\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\quang\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:2454\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2454\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    650\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[0;32m    354\u001b[0m     tensors,\n\u001b[0;32m    355\u001b[0m     grad_tensors_,\n\u001b[0;32m    356\u001b[0m     retain_graph,\n\u001b[0;32m    357\u001b[0m     create_graph,\n\u001b[0;32m    358\u001b[0m     inputs,\n\u001b[0;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d859d986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: titles, tokens. If titles, tokens are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1267\n",
      "  Batch size = 32\n",
      "c:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.359478235244751,\n",
       " 'eval_model_preparation_time': 0.002,\n",
       " 'eval_Validation Accuracy': 0.8389897395422258,\n",
       " 'eval_Validation Precision': 0.838791277258567,\n",
       " 'eval_Validation AUC': 0.8328267477203647,\n",
       " 'eval_Validation Recall': 0.8535825545171339,\n",
       " 'eval_Validation F1_Score': 0.8430769230769231,\n",
       " 'eval_Validation TP': 548,\n",
       " 'eval_Validation FP': 110,\n",
       " 'eval_Validation FN': 94,\n",
       " 'eval_Validation TN': 515,\n",
       " 'eval_runtime': 14.7476,\n",
       " 'eval_samples_per_second': 85.912,\n",
       " 'eval_steps_per_second': 2.712,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d0100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./distilbert_news_clf/results\n",
      "Configuration saved in ./distilbert_news_clf/results\\config.json\n",
      "Model weights saved in ./distilbert_news_clf/results\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./distilbert_news_clf/results\\tokenizer_config.json\n",
      "Special tokens file saved in ./distilbert_news_clf/results\\special_tokens_map.json\n",
      "Configuration saved in ./distilbert-base-uncased\\config.json\n",
      "Model weights saved in ./distilbert-base-uncased\\model.safetensors\n",
      "tokenizer config file saved in ./distilbert-base-uncased\\tokenizer_config.json\n",
      "Special tokens file saved in ./distilbert-base-uncased\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./distilbert-base-uncased\\\\tokenizer_config.json',\n",
       " './distilbert-base-uncased\\\\special_tokens_map.json',\n",
       " './distilbert-base-uncased\\\\vocab.txt',\n",
       " './distilbert-base-uncased\\\\added_tokens.json',\n",
       " './distilbert-base-uncased\\\\tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "model_path = \"./distilbert-base-uncased\"\n",
    "sequence_clf_model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f33d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./distilbert-base-uncased\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"REAL\",\n",
      "    \"1\": \"FAKE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": null,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file ./distilbert-base-uncased\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"REAL\",\n",
      "    \"1\": \"FAKE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": null,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./distilbert-base-uncased\\model.safetensors\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", \"./distilbert-base-uncased\", tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2051e592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'FAKE', 'score': 0.922602653503418}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"skibidi toilet just died\"\n",
    "pipe(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
