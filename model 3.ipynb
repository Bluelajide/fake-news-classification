{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f26d8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import torch\n",
    "from transformers.file_utils import is_tf_available, is_torch_available\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, RobertaTokenizerFast, RobertaForSequenceClassification, BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58d7d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6335, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6903</td>\n",
       "      <td>Tehran, USA</td>\n",
       "      <td>\\nI’m not an immigrant, but my grandparents ...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7341</td>\n",
       "      <td>Girl Horrified At What She Watches Boyfriend D...</td>\n",
       "      <td>Share This Baylee Luciani (left), Screenshot o...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>95</td>\n",
       "      <td>‘Britain’s Schindler’ Dies at 106</td>\n",
       "      <td>A Czech stockbroker who saved more than 650 Je...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4869</td>\n",
       "      <td>Fact check: Trump and Clinton at the 'commande...</td>\n",
       "      <td>Hillary Clinton and Donald Trump made some ina...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2909</td>\n",
       "      <td>Iran reportedly makes new push for uranium con...</td>\n",
       "      <td>Iranian negotiators reportedly have made a las...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1357</td>\n",
       "      <td>With all three Clintons in Iowa, a glimpse at ...</td>\n",
       "      <td>CEDAR RAPIDS, Iowa — “I had one of the most wo...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>988</td>\n",
       "      <td>Donald Trump’s Shockingly Weak Delegate Game S...</td>\n",
       "      <td>Donald Trump’s organizational problems have go...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7041</td>\n",
       "      <td>Strong Solar Storm, Tech Risks Today | S0 News...</td>\n",
       "      <td>Click Here To Learn More About Alexandra's Per...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7623</td>\n",
       "      <td>10 Ways America Is Preparing for World War 3</td>\n",
       "      <td>October 31, 2016 at 4:52 am \\nPretty factual e...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1571</td>\n",
       "      <td>Trump takes on Cruz, but lightly</td>\n",
       "      <td>Killing Obama administration rules, dismantlin...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                              title  \\\n",
       "0         8476                       You Can Smell Hillary’s Fear   \n",
       "1        10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2         3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3        10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4          875   The Battle of New York: Why This Primary Matters   \n",
       "5         6903                                        Tehran, USA   \n",
       "6         7341  Girl Horrified At What She Watches Boyfriend D...   \n",
       "7           95                  ‘Britain’s Schindler’ Dies at 106   \n",
       "8         4869  Fact check: Trump and Clinton at the 'commande...   \n",
       "9         2909  Iran reportedly makes new push for uranium con...   \n",
       "10        1357  With all three Clintons in Iowa, a glimpse at ...   \n",
       "11         988  Donald Trump’s Shockingly Weak Delegate Game S...   \n",
       "12        7041  Strong Solar Storm, Tech Risks Today | S0 News...   \n",
       "13        7623       10 Ways America Is Preparing for World War 3   \n",
       "14        1571                   Trump takes on Cruz, but lightly   \n",
       "\n",
       "                                                 text label  \n",
       "0   Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1   Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2   U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3   — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4   It's primary day in New York and front-runners...  REAL  \n",
       "5     \\nI’m not an immigrant, but my grandparents ...  FAKE  \n",
       "6   Share This Baylee Luciani (left), Screenshot o...  FAKE  \n",
       "7   A Czech stockbroker who saved more than 650 Je...  REAL  \n",
       "8   Hillary Clinton and Donald Trump made some ina...  REAL  \n",
       "9   Iranian negotiators reportedly have made a las...  REAL  \n",
       "10  CEDAR RAPIDS, Iowa — “I had one of the most wo...  REAL  \n",
       "11  Donald Trump’s organizational problems have go...  REAL  \n",
       "12  Click Here To Learn More About Alexandra's Per...  FAKE  \n",
       "13  October 31, 2016 at 4:52 am \\nPretty factual e...  FAKE  \n",
       "14  Killing Obama administration rules, dismantlin...  REAL  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_news = pd.read_csv('fake_or_real_news.csv')\n",
    "print(data_news.shape)\n",
    "data_news[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4a1685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code segment parses the data_news dataset into a more manageable format\n",
    "\n",
    "titles = []\n",
    "tokenized_titles = []\n",
    "sequence_labels = data_news['label']\n",
    "\n",
    "title, tokenized_title =  [], []\n",
    "for news in data_news['title']:\n",
    "    title.append(news)\n",
    "    tokenized_title.append(news.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf5855b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('You Can Smell Hillary’s Fear',\n",
       " ['You', 'Can', 'Smell', 'Hillary’s', 'Fear'],\n",
       " 'FAKE')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python list for each news\n",
    "title[0], tokenized_title[0], sequence_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83b44808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FAKE', 'REAL']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sequence_labels = list(set(sequence_labels))\n",
    "unique_sequence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a93b4893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'titles': 'Get Ready For A Fight To Replace Scalia',\n",
       " 'label': 1,\n",
       " 'tokens': ['Get', 'Ready', 'For', 'A', 'Fight', 'To', 'Replace', 'Scalia']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert sequence_labels to indices:\n",
    "label_indices = [unique_sequence_labels.index(l) for l in sequence_labels]\n",
    "\n",
    "news_dataset = Dataset.from_dict(\n",
    "    dict(\n",
    "        titles=title, \n",
    "        label=label_indices,\n",
    "        tokens=tokenized_title,\n",
    "    )\n",
    ")\n",
    "news_dataset = news_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "news_dataset['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53039ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base-clf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0af3ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"titles\"], truncation=True, padding=True)# truncation=True makes sure to exludes instances with more 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be8540e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fb30612edb4300a714276ff5ce909f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40dfa8c9a6ab4e4ca7b6f8c2fde7a97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# go over all our data set, tokenize them\n",
    "seq_clf_tokenized_news = news_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f5f051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'titles': 'Get Ready For A Fight To Replace Scalia',\n",
       " 'label': 1,\n",
       " 'tokens': ['Get', 'Ready', 'For', 'A', 'Fight', 'To', 'Replace', 'Scalia'],\n",
       " 'input_ids': [0,\n",
       "  14181,\n",
       "  19325,\n",
       "  286,\n",
       "  83,\n",
       "  14381,\n",
       "  598,\n",
       "  42439,\n",
       "  31462,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_clf_tokenized_news['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c7d1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31e249f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sequence_clf_model = RobertaForSequenceClassification.from_pretrained('roberta-base', \n",
    "                                                                         num_labels=len(unique_sequence_labels),)\n",
    "\n",
    "# set an index -> label dictionary\n",
    "sequence_clf_model.config.id2label = {i: l for i, l in enumerate(unique_sequence_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c4d73df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"FAKE\",\n",
       "    \"1\": \"REAL\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.51.3\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_clf_model.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "899bf63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):  # common method to take in logits and calculate accuracy of the eval set\n",
    "    logits, labels = eval_pred   # logit and label are returning from training loop\n",
    "    predictions = np.argmax(logits, axis=-1) \n",
    "    return metric.compute(predictions=predictions, references=labels) # compute the accuracy\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "def compute_metrics_binary(eval_pred):\n",
    "    \"\"\"metrics for binary classification\"\"\"\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Calculate the AUC score\n",
    "    auc_score = roc_auc_score(labels, preds)\n",
    "\n",
    "    # Calculate the accuracy, true positive, false positive, false negative, and true negative values\n",
    "    acc = metric.compute(predictions=preds, references=labels)\n",
    "    tp = ((preds >= 0.5) & (labels == 1)).sum()\n",
    "    fp = ((preds >= 0.5) & (labels == 0)).sum()\n",
    "    fn = ((preds < 0.5) & (labels == 1)).sum()\n",
    "    tn = ((preds < 0.5) & (labels == 0)).sum()\n",
    "\n",
    "    # Calculate the precision, recall, and F1 score\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return {\n",
    "        'Validation Accuracy': acc['accuracy'],\n",
    "        'Validation Precision': auc_score,\n",
    "        'Validation AUC': precision,\n",
    "        'Validation Recall': recall,\n",
    "        'Validation F1_Score': f1_score,\n",
    "        'Validation TP': tp,\n",
    "        'Validation FP': fp,\n",
    "        'Validation FN': fn,\n",
    "        'Validation TN': tn,\n",
    "    }\n",
    "\n",
    "#####################################################\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_metrics_multiclass(eval_pred):\n",
    "    \"\"\"metrics for multiclass classification\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    report = classification_report(labels, preds, output_dict=True)\n",
    "    acc_score = report['accuracy']\n",
    "    pre_score = report['macro avg']['precision']\n",
    "    rcl_score = report['macro avg']['recall']\n",
    "    f1_score = report['macro avg']['f1-score']\n",
    "\n",
    "    return {\n",
    "        'Validation Accuracy': acc_score,\n",
    "        'Validation Macro Recall': rcl_score,\n",
    "        'Validation Macro Precision': pre_score,        \n",
    "        'Validation Macro F1_Score': f1_score,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "# Training argument\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta_news_clf/results\", # Local directory to save check point of our model as fitting\n",
    "    num_train_epochs=epochs,         # minimum of two epochs\n",
    "    per_device_train_batch_size=32,  # batch size for training and evaluation, it common to take around 32, \n",
    "    per_device_eval_batch_size=32,   # sometimes less or more, The smaller batch size, the more change model update \n",
    "    load_best_model_at_end=True,     # Even if we overfit the model by accident, load the best model through checkpoint\n",
    "    \n",
    "    # some deep learning parameters that the trainer is able to take in\n",
    "    warmup_steps = len(seq_clf_tokenized_news['train']) // 5,  # learning rate scheduler by number of warmup steps\n",
    "    weight_decay = 0.05,    # weight decay for our learning rate schedule (regularization)\n",
    "    \n",
    "    logging_steps = 1,  # Tell the model minimum number of steps to log between (1 means logging as much as possible)\n",
    "    log_level = 'info',\n",
    "    eval_strategy = 'epoch', # It is \"steps\" or \"epoch\", we choose epoch: how many times to stop training to test\n",
    "    eval_steps = 50,\n",
    "    save_strategy = 'epoch',  # save a check point of our model after each epoch\n",
    "    learning_rate=2e-5,  # learning rate for our model\n",
    ")\n",
    "\n",
    "# Define the trainer:\n",
    "trainer = Trainer(\n",
    "    model=sequence_clf_model,   # take our model (sequence_clf_model)\n",
    "    args=training_args,         # we just set it above\n",
    "    train_dataset=seq_clf_tokenized_news['train'], # training part of dataset\n",
    "    eval_dataset=seq_clf_tokenized_news['test'],   # test (evaluation) part of dataset\n",
    "    compute_metrics=compute_metrics_binary,    # This part is optional but we want to calculate accuracy of our model \n",
    "    data_collator=data_collator         # data colladior with padding. Infact, we may or may not need a data collator\n",
    "                                        # we can check the model to see how it lookes like with or without the collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57f6d7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tokens, titles. If tokens, titles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1267\n",
      "  Batch size = 32\n",
      "c:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 18:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_30164\\3547021378.py:39: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = tp / (tp + fp)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7036638855934143,\n",
       " 'eval_model_preparation_time': 0.003,\n",
       " 'eval_Validation Accuracy': 0.47829518547750594,\n",
       " 'eval_Validation Precision': 0.5,\n",
       " 'eval_Validation AUC': nan,\n",
       " 'eval_Validation Recall': 0.0,\n",
       " 'eval_Validation F1_Score': nan,\n",
       " 'eval_Validation TP': 0,\n",
       " 'eval_Validation FP': 0,\n",
       " 'eval_Validation FN': 661,\n",
       " 'eval_Validation TN': 606,\n",
       " 'eval_runtime': 33.8481,\n",
       " 'eval_samples_per_second': 37.432,\n",
       " 'eval_steps_per_second': 1.182}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get initial metrics: evaluation on test set\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42f93922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tokens, titles. If tokens, titles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 5,068\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,590\n",
      "  Number of trainable parameters = 124,647,170\n",
      "c:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1590' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1590/1590 3:02:52, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Validation accuracy</th>\n",
       "      <th>Validation precision</th>\n",
       "      <th>Validation auc</th>\n",
       "      <th>Validation recall</th>\n",
       "      <th>Validation f1 Score</th>\n",
       "      <th>Validation tp</th>\n",
       "      <th>Validation fp</th>\n",
       "      <th>Validation fn</th>\n",
       "      <th>Validation tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>0.317413</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.865036</td>\n",
       "      <td>0.864403</td>\n",
       "      <td>0.864583</td>\n",
       "      <td>0.878971</td>\n",
       "      <td>0.871718</td>\n",
       "      <td>581</td>\n",
       "      <td>91</td>\n",
       "      <td>80</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.705300</td>\n",
       "      <td>0.280540</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.891871</td>\n",
       "      <td>0.893142</td>\n",
       "      <td>0.923948</td>\n",
       "      <td>0.863843</td>\n",
       "      <td>0.892885</td>\n",
       "      <td>571</td>\n",
       "      <td>47</td>\n",
       "      <td>90</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.272720</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.899763</td>\n",
       "      <td>0.898579</td>\n",
       "      <td>0.886957</td>\n",
       "      <td>0.925870</td>\n",
       "      <td>0.905996</td>\n",
       "      <td>612</td>\n",
       "      <td>78</td>\n",
       "      <td>49</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.305627</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.900552</td>\n",
       "      <td>0.900365</td>\n",
       "      <td>0.904690</td>\n",
       "      <td>0.904690</td>\n",
       "      <td>0.904690</td>\n",
       "      <td>598</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.355812</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.905288</td>\n",
       "      <td>0.904148</td>\n",
       "      <td>0.892598</td>\n",
       "      <td>0.930408</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>615</td>\n",
       "      <td>74</td>\n",
       "      <td>46</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.383048</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.892660</td>\n",
       "      <td>0.890260</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.945537</td>\n",
       "      <td>0.901876</td>\n",
       "      <td>625</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.593600</td>\n",
       "      <td>0.629158</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.841358</td>\n",
       "      <td>0.834982</td>\n",
       "      <td>0.774463</td>\n",
       "      <td>0.981846</td>\n",
       "      <td>0.865911</td>\n",
       "      <td>649</td>\n",
       "      <td>189</td>\n",
       "      <td>12</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.595834</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.903710</td>\n",
       "      <td>0.901811</td>\n",
       "      <td>0.879044</td>\n",
       "      <td>0.945537</td>\n",
       "      <td>0.911079</td>\n",
       "      <td>625</td>\n",
       "      <td>86</td>\n",
       "      <td>36</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.601822</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.902920</td>\n",
       "      <td>0.901467</td>\n",
       "      <td>0.885387</td>\n",
       "      <td>0.934947</td>\n",
       "      <td>0.909492</td>\n",
       "      <td>618</td>\n",
       "      <td>80</td>\n",
       "      <td>43</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.620531</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.907656</td>\n",
       "      <td>0.906349</td>\n",
       "      <td>0.891931</td>\n",
       "      <td>0.936460</td>\n",
       "      <td>0.913653</td>\n",
       "      <td>619</td>\n",
       "      <td>75</td>\n",
       "      <td>42</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tokens, titles. If tokens, titles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1267\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./roberta_news_clf/results\\checkpoint-159\n",
      "Configuration saved in ./roberta_news_clf/results\\checkpoint-159\\config.json\n",
      "Model weights saved in ./roberta_news_clf/results\\checkpoint-159\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./roberta_news_clf/results\\checkpoint-159\\tokenizer_config.json\n",
      "Special tokens file saved in ./roberta_news_clf/results\\checkpoint-159\\special_tokens_map.json\n",
      "c:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tokens, titles. If tokens, titles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1267\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./roberta_news_clf/results\\checkpoint-318\n",
      "Configuration saved in ./roberta_news_clf/results\\checkpoint-318\\config.json\n",
      "Model weights saved in ./roberta_news_clf/results\\checkpoint-318\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./roberta_news_clf/results\\checkpoint-318\\tokenizer_config.json\n",
      "Special tokens file saved in ./roberta_news_clf/results\\checkpoint-318\\special_tokens_map.json\n",
      "c:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tokens, titles. If tokens, titles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1267\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./roberta_news_clf/results\\checkpoint-477\n",
      "Configuration saved in ./roberta_news_clf/results\\checkpoint-477\\config.json\n",
      "Model weights saved in ./roberta_news_clf/results\\checkpoint-477\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./roberta_news_clf/results\\checkpoint-477\\tokenizer_config.json\n",
      "Special tokens file saved in ./roberta_news_clf/results\\checkpoint-477\\special_tokens_map.json\n",
      "c:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tokens, titles. If tokens, titles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1267\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./roberta_news_clf/results\\checkpoint-636\n",
      "Configuration saved in ./roberta_news_clf/results\\checkpoint-636\\config.json\n",
      "Model weights saved in ./roberta_news_clf/results\\checkpoint-636\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./roberta_news_clf/results\\checkpoint-636\\tokenizer_config.json\n",
      "Special tokens file saved in ./roberta_news_clf/results\\checkpoint-636\\special_tokens_map.json\n",
      "c:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tokens, titles. If tokens, titles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1267\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./roberta_news_clf/results\\checkpoint-795\n",
      "Configuration saved in ./roberta_news_clf/results\\checkpoint-795\\config.json\n",
      "Model weights saved in ./roberta_news_clf/results\\checkpoint-795\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./roberta_news_clf/results\\checkpoint-795\\tokenizer_config.json\n",
      "Special tokens file saved in ./roberta_news_clf/results\\checkpoint-795\\special_tokens_map.json\n",
      "c:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tokens, titles. If tokens, titles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1267\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./roberta_news_clf/results\\checkpoint-954\n",
      "Configuration saved in ./roberta_news_clf/results\\checkpoint-954\\config.json\n",
      "Model weights saved in ./roberta_news_clf/results\\checkpoint-954\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./roberta_news_clf/results\\checkpoint-954\\tokenizer_config.json\n",
      "Special tokens file saved in ./roberta_news_clf/results\\checkpoint-954\\special_tokens_map.json\n",
      "c:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tokens, titles. If tokens, titles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1267\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./roberta_news_clf/results\\checkpoint-1113\n",
      "Configuration saved in ./roberta_news_clf/results\\checkpoint-1113\\config.json\n",
      "Model weights saved in ./roberta_news_clf/results\\checkpoint-1113\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./roberta_news_clf/results\\checkpoint-1113\\tokenizer_config.json\n",
      "Special tokens file saved in ./roberta_news_clf/results\\checkpoint-1113\\special_tokens_map.json\n",
      "c:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tokens, titles. If tokens, titles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1267\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./roberta_news_clf/results\\checkpoint-1272\n",
      "Configuration saved in ./roberta_news_clf/results\\checkpoint-1272\\config.json\n",
      "Model weights saved in ./roberta_news_clf/results\\checkpoint-1272\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./roberta_news_clf/results\\checkpoint-1272\\tokenizer_config.json\n",
      "Special tokens file saved in ./roberta_news_clf/results\\checkpoint-1272\\special_tokens_map.json\n",
      "c:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tokens, titles. If tokens, titles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1267\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./roberta_news_clf/results\\checkpoint-1431\n",
      "Configuration saved in ./roberta_news_clf/results\\checkpoint-1431\\config.json\n",
      "Model weights saved in ./roberta_news_clf/results\\checkpoint-1431\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./roberta_news_clf/results\\checkpoint-1431\\tokenizer_config.json\n",
      "Special tokens file saved in ./roberta_news_clf/results\\checkpoint-1431\\special_tokens_map.json\n",
      "c:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tokens, titles. If tokens, titles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1267\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./roberta_news_clf/results\\checkpoint-1590\n",
      "Configuration saved in ./roberta_news_clf/results\\checkpoint-1590\\config.json\n",
      "Model weights saved in ./roberta_news_clf/results\\checkpoint-1590\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./roberta_news_clf/results\\checkpoint-1590\\tokenizer_config.json\n",
      "Special tokens file saved in ./roberta_news_clf/results\\checkpoint-1590\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./roberta_news_clf/results\\checkpoint-477 (score: 0.2727195918560028).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1590, training_loss=0.18001216948024143, metrics={'train_runtime': 10981.5302, 'train_samples_per_second': 4.615, 'train_steps_per_second': 0.145, 'total_flos': 2421045153666720.0, 'train_loss': 0.18001216948024143, 'epoch': 10.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d859d986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tokens, titles. If tokens, titles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1267\n",
      "  Batch size = 32\n",
      "c:\\Users\\quang\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2727195918560028,\n",
       " 'eval_model_preparation_time': 0.003,\n",
       " 'eval_Validation Accuracy': 0.8997632202052092,\n",
       " 'eval_Validation Precision': 0.89857851140636,\n",
       " 'eval_Validation AUC': 0.8869565217391304,\n",
       " 'eval_Validation Recall': 0.9258698940998488,\n",
       " 'eval_Validation F1_Score': 0.9059955588452998,\n",
       " 'eval_Validation TP': 612,\n",
       " 'eval_Validation FP': 78,\n",
       " 'eval_Validation FN': 49,\n",
       " 'eval_Validation TN': 528,\n",
       " 'eval_runtime': 37.6921,\n",
       " 'eval_samples_per_second': 33.614,\n",
       " 'eval_steps_per_second': 1.061,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92d0100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./roberta_news_clf/results\n",
      "Configuration saved in ./roberta_news_clf/results\\config.json\n",
      "Model weights saved in ./roberta_news_clf/results\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./roberta_news_clf/results\\tokenizer_config.json\n",
      "Special tokens file saved in ./roberta_news_clf/results\\special_tokens_map.json\n",
      "Configuration saved in ./roberta-base-clf\\config.json\n",
      "Model weights saved in ./roberta-base-clf\\model.safetensors\n",
      "tokenizer config file saved in ./roberta-base-clf\\tokenizer_config.json\n",
      "Special tokens file saved in ./roberta-base-clf\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./roberta-base-clf\\\\tokenizer_config.json',\n",
       " './roberta-base-clf\\\\special_tokens_map.json',\n",
       " './roberta-base-clf\\\\vocab.json',\n",
       " './roberta-base-clf\\\\merges.txt',\n",
       " './roberta-base-clf\\\\added_tokens.json',\n",
       " './roberta-base-clf\\\\tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "model_path = \"./roberta-base-clf\"\n",
    "sequence_clf_model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08f33d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./roberta-base\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"FAKE\",\n",
      "    \"1\": \"REAL\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": null,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file ./roberta-base\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"FAKE\",\n",
      "    \"1\": \"REAL\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": null,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./roberta-base\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", \"./roberta-base\", tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2051e592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'FAKE', 'score': 0.9685542583465576}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"skibidi toilet just died\"\n",
    "pipe(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
